%
\section{Related Work}
\label{sec:related-work}
%
% literature
Ma et al. \cite{ma_efficient_2019} propose a framework for efficient live migration of edge services using Docker containers.
They reduce migration time by leveraging Docker's layered storage architecture.
This is possible by only transferring the top writable layer and downloading the remaining image from a known repository. This drastically decreases the amount of data transferred from one environment to another.
Their approach assumes that the service is encapsulated within a single container and focuses primarily on reducing file system and memory transfer size during migration.
However, migrating single containers does not meet our requirements, as we focus on orchestrated environments such as Kubernetes clusters, which involve, for example, multiple interconnected pods, services or volumes across the system.
Migrating such an application stack requires additional consistency in orchestration, resource management, and internal networking that the container-only approach does not address.
In addition there is also the question about consistency while transferring the data.
Since the work focuses on single-container migrations, it does not address orchestrated multi-container setups or stateful applications that require persistent state management and complex networking.

Similarly, Kaur et al. \cite{kaur_live_2023} researched the live migration of containerized microservices across Kubernetes clusters. Their example involves a 5G Mobile Core Network, which consists of several pods and services deployed in a cloud-fog-edge setup. To place latency-affected applications close to the user, they migrate less critical applications from the edge to the cloud. This helps to free up resources while maintaining service availability. They handle their own traffic rerouting to ensure that the connection between two applications is not lost during migration. They use Traefik ingress controllers for routing and an external DNS server for hostname redirection to ensure uninterrupted communication between these applications during migration. Their migration process involves four steps: deploying the application on the target cluster, configuring a new ingress route, updating the DNS entry, and removing the original instance. They implemented the process so that the migration is triggered based on metrics collected by Prometheus. While this approach seems effective, it depends on manual configuration and is also targeted at stateless workloads. Their work does not deal with automating the migration, handling stateful applications, or managing dynamic resource allocation in orchestrated environments.

Barbarulo et al. address the challenge of migrating stateful applications running in containers in edge computing environments.They refer to and base their implementation  on the Multi-access Edge Computing (MEC) standards defined by the European Telecommunications Standards Institute (ETSI). Their work focuses on guaranteeing service continuity, which includes both rerouting traffic to the correct endpoints and the migration of the application state, including memory, disk content, or variable values at the application level. In their migration  process, they distinguish between stateless relocation, which involves deploying the application elsewhere, and stateful relocation, which requires additionally transferring the state of the application. The authors extend the MEC architecture by using container migration techniques. This helps to support transparent stateful application relocation. This helps to support transparent stateful application relocation. Applications are running in containers on MEC hosts, which are managed by a self-implemented orchestrator and a Virtualization Infrastructure Manager, which allocates resources and manages the infrastructure. They outline three different approaches to transfer the user context: application self-controlled, device application-assisted, and MEC-assisted. The application self-controlled approach handles its own state transfer. In the device application-assisted approach, the user device coordinates migration and informs the client application. The MEC-assisted approach allows the MEC system to notify the source application to start the transfer, but the application still handles the serialization and restoration by itself. In all cases, however, the application remains responsible for the stateful relocation by serializing and restoring the state. Handling state relocation within the application can be challenging, requiring developers to implement complex logic. Migrating low-level runtime data causes the running container to freeze and transfer all of its memory, resulting in service downtime of up to 13 seconds for the test application. In addition, the approach only allows cold migrations, which means that the service is temporarily interrupted. The state transfer is a stop-and-copy approach (no live migration), and serialization can be slow and does not work for low-level states. The approach also lacks state transfer security considerations and does not address database state migration.

% tools
Looking into popular real-world solutions that provide migration capabilities, we stumbled upon Velero. \footnote{\url{https://velero.io/}} "Velero is an open-source tool to safely back up and restore, perform disaster recovery, and migrate Kubernetes cluster resources and persistent volumes." Velero works for on-premise and cloud environments and operates at the cluster level. It uses the Kubernetes API to capture the state of the cluster and thus enables restoring or migrating the state. By using the Kubernetes API, it provides a high degree of flexibility. It also allows capturing a subset of resources by filtering based on namespace, resource type, or labels. Velero is a highly popular and stable tool to migrate Kubernetes resources, but it has its downsides. We need an S3-compliant object storage system to use Velero for our migration. This is necessary because we need to back up our cluster first and then migrate it. For the on-premise scenario, we also have to properly configure the S3-compatible storage and enable network access between both clusters and the storage. A common pain point involves firewall and DNS issues. Another critical point is that Velero does not support live migration. Velero's goal isn't seamless migration. The origin cluster must be paused while backing up the data to the target cluster to avoid inconsistencies in the backup data. This is problematic because it backs up data to storage and does not replicate the live state of the database. For the on-premise scenario, we have networking and DNS differences between the two clusters. For example, service IP ranges, DNS configuration, internal references, and ingress rules must be manually patched in most cases after migration.

% sum up
The objective of this research is to address these limitations through the development of an automated tool for stateful and stateless migration in orchestrated edge systems. To ensure minimal downtime and consistent performance during migrations, the proposed solution focuses on seamless state management and real-time request forwarding.
%
\subsection{Cloud edge orchestration}
%
Cloud-edge orchestration involves managing and assigning workloads across the cloud, edge, and IoT layers to meet objectives like efficient application placement, low latency, and resource optimization. It addresses the complexities of distributed, heterogeneous, and large-scale systems by employing decision models, container technology, and security mechanisms to ensure scalability, fault tolerance, and ease of deployment. Orchestrators dynamically allocate containers to nodes based on strategies and policies to achieve seamless operation across the architecture.\cite{bohm_cloud-edge_2022}
%
\subsubsection{Offloading}
Offloading refers to the process of moving computing tasks, applications, or data closer to the edge of the network to optimize performance, reduce latency, conserve energy, and make more efficient use of available resources. This can be done in two main ways: offloading from user devices to the edge and offloading from the cloud to the edge. User-device offloading moves tasks from devices with limited compute, memory, or power to nearby edge nodes. Cloud-to-edge offloading moves workloads from the cloud to edge nodes to reduce latency and improve performance by moving processing closer to the source.\cite{hong_resource_2020} 
%
\subsubsection{Scaling}
Scaling, also known as distributed offloading, is a collaborative provision model that occurs between the edge and the cloud. In this approach, tasks and resources are shared between the edge and the cloud and vice versa to meet specific requirements. Unlike traditional offloading, where tasks are completely moved from the edge to the cloud, scaling allows the edge and cloud to share data and resources during service execution. This leverages the strengths of both, with the edge handling low-latency tasks and the cloud handling compute-intensive tasks. By working together in a distributed manner, the edge and the cloud can adaptively meet application needs while optimizing performance and accuracy. Once the offloaded tasks have been computed, the offloading layer (edge or cloud) completes service execution.\cite{aazam_offloading_2018}
%
\subsubsection{Service placement}
Service placement is a key challenge in edge and fog computing. In edge cloud computing, it involves determining the optimal location to deploy and execute services or applications within a distributed network. The objective is to identify the placement that minimizes latency, energy consumption, and bandwidth usage while maximizing resource availability and ensuring good network conditions. This decision is typically dynamic, as factors such as load, the number of edge devices, and their positions can change over time. These variations may make previously optimal placements inefficient, requiring the migration of services to alternative servers to maintain optimal performance.\cite{zheng_service_2024}